{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf7d772-e6e4-463c-b09c-5b82071a988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as pd\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e3c066-c41c-4b22-811b-0a3c34dc7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDataset(Dataset):\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.data = ImageFolder(dir_path, transform = transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b832c029-b060-4cd2-ad65-31a969700476",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7432bc16-9505-4d23-95b9-7c1c9bbcc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/aul/homes/amaha038/DeepLearning/Datasets/Card_Dataset_Kaggle/train/'\n",
    "val_path = '/aul/homes/amaha038/DeepLearning/Datasets/Card_Dataset_Kaggle/valid/'\n",
    "\n",
    "train_data = CardDataset(train_path, transform = transform)\n",
    "val_data = CardDataset(val_path, transform = transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 32, num_workers=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size = 32, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9531439b-0893-46ec-aced-5a53fe9649df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Note on Rearrange from Einops\\nInputsize = x: [B, C, H, W], let's say H = 224 and C = 3.\\n1. Say, patch_size = 4\\nNow, h = 224//4 = 56, w = 224//4 = 56\\n2. B C H W = b c (h p1) (w p2), because b = B, c = C, h*p1 = 224, w*p2 = 224\\n3. Now, Reshaping: b c (h p1) (w p2) -> b (h w) (p1 p2 c)\\nWhat we get: (h w) = number of patches (flattened to one dimension)\\n             (p1 p2 c): patch pixels + channels flattened into a vector = **PatchVectorSize**\\n4. So, we get, [B, 3136, 4*4*C] = [B, NumPatches, PatchVectorSize]. \\n    Now, we have sequence of 3136 flattened patches\\n5. We have to project each patch to certain embedding.\\n   Let's say the emb_size = embedding dimension\\n   Now, we apply, nn.Linear(PatchVectorSize, emb_size).\\n   Finally, each tensor will be of dimension = [Batch, NumPatches, emb_size]\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"Note on Rearrange from Einops\n",
    "Inputsize = x: [B, C, H, W], let's say H = 224 and C = 3.\n",
    "1. Say, patch_size = 4\n",
    "Now, h = 224//4 = 56, w = 224//4 = 56\n",
    "2. B C H W = b c (h p1) (w p2), because b = B, c = C, h*p1 = 224, w*p2 = 224\n",
    "3. Now, Reshaping: b c (h p1) (w p2) -> b (h w) (p1 p2 c)\n",
    "What we get: (h w) = number of patches (flattened to one dimension)\n",
    "             (p1 p2 c): patch pixels + channels flattened into a vector = **PatchVectorSize**\n",
    "4. So, we get, [B, 3136, 4*4*C] = [B, NumPatches, PatchVectorSize]. \n",
    "    Now, we have sequence of 3136 flattened patches\n",
    "5. We have to project each patch to certain embedding.\n",
    "   Let's say the emb_size = embedding dimension\n",
    "   Now, we apply, nn.Linear(PatchVectorSize, emb_size).\n",
    "   Finally, each tensor will be of dimension = [Batch, NumPatches, emb_size]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584185f-c795-4271-9c23-9118b4f527d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6aff9a-aaf6-4f31-9951-2a1e4b3d4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = nn.MultiheadAttention(embed_dim=dim,\n",
    "                                         num_heads=n_heads,\n",
    "                                         dropout=dropout)\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x): # x: [batch, seq_len, dim], we had this from the PatchEmbedding\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch, dim] as expected by multiheadattention\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, _ = self.att(q, k, v)\n",
    "        return attn_output.transpose(0, 1)  # [batch, seq_len, dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b570d9-10bb-45dc-98f4-87408a243101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better readable multihead attention\n",
    "\"\"\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=dim,\n",
    "                                         num_heads=n_heads,\n",
    "                                         dropout=dropout,\n",
    "                                         batch_first=True) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, dim] because batch_first=True\n",
    "        attn_output, _ = self.att(x, x, x)  # Q=K=V=x\n",
    "        return attn_output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03198c8b-0cd7-47a6-9810-b37acb7ba0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b35c06-2808-489d-87d5-aed83b4b4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2eefd44-0f83-47b0-bdc3-cce69d18a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047b33bf-bd9c-48ed-b292-3cf8274aa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit: https://www.youtube.com/watch?v=j3VNqtJUoz0&list=PLcpLsgRAryqx-dwIuJ9tT6BxJu8__LUUW&index=3\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, ch=3, img_size=224, patch_size=4, emb_dim=32,\n",
    "                n_layers=6, out_dim=53, dropout=0.1, heads=2): #out_dim=53, matching the number of classes\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
    "                                              patch_size=patch_size,\n",
    "                                              emb_size=emb_dim)\n",
    "        # Learnable params\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token to inputs\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "\n",
    "        # Output based on classification token\n",
    "        return self.head(x[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfef38c5-8c93-4d6a-9baa-3ee94eb8400e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (projection): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
       "      (1): Linear(in_features=48, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x Sequential(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (att): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "            )\n",
       "            (q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (k): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (v): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=32, out_features=53, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4700782-5083-4393-9682-a12e04c86e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683322c-6424-462f-8ca6-98634f822187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c55957486c4cef93cbf5964c30054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58c7d7f0d443faa33bedc77734b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 - Train loss: 3.946616685653158, Val Loss: 3.683026289490034. Train Acc.: 0.026888772298006295, Val Acc.: 0.05660377358490566\n",
      "The model weight is saved based on val_acc: 0.0566\n",
      "The model weight is saved based on val_loss: 3.6830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b21d4dce2ef48cbb43475f8b5ddafd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43da6800488450ead01789cf6bf176f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150 - Train loss: 3.3724719097081413, Val Loss: 2.833555831549303. Train Acc.: 0.12631164742917103, Val Acc.: 0.2528301886792453\n",
      "The model weight is saved based on val_acc: 0.2528\n",
      "The model weight is saved based on val_loss: 2.8336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e5d43ba2e4a43bee0f7f441eb33d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd5ae2a9511489ebdd730a109f26230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150 - Train loss: 2.8426398863197, Val Loss: 2.4806122761852336. Train Acc.: 0.2296694648478489, Val Acc.: 0.27169811320754716\n",
      "The model weight is saved based on val_acc: 0.2717\n",
      "The model weight is saved based on val_loss: 2.4806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bec105093ef4dc983e736df6008f00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d853c031274de08c1e67891234f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/150 - Train loss: 2.586635190067111, Val Loss: 2.2428677536406605. Train Acc.: 0.26285414480587616, Val Acc.: 0.3320754716981132\n",
      "The model weight is saved based on val_acc: 0.3321\n",
      "The model weight is saved based on val_loss: 2.2429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a7170e74df4bcbb7830efac413f568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f5d076b3a4899a8ec3ab3e2eedc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/150 - Train loss: 2.4479025858772014, Val Loss: 2.2033963743245826. Train Acc.: 0.287906610703043, Val Acc.: 0.2943396226415094\n",
      "The model weight is saved based on val_loss: 2.2034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec7050ee1314a118be3a7188e1e24a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f18ae667924c26af71543c52d97d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/150 - Train loss: 2.342922113374548, Val Loss: 2.015141246453771. Train Acc.: 0.3074501573976915, Val Acc.: 0.30566037735849055\n",
      "The model weight is saved based on val_loss: 2.0151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50c53f3336f4515b36c0a5f280194b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59c37e5fc1f4a7fbffaa4d5c0dcdc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/150 - Train loss: 2.256357228268106, Val Loss: 1.897922255408089. Train Acc.: 0.3277806925498426, Val Acc.: 0.3660377358490566\n",
      "The model weight is saved based on val_acc: 0.3660\n",
      "The model weight is saved based on val_loss: 1.8979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7806c94c6c82423c98d73957ac591096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81a37f06a724efcabacee42477320ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150 - Train loss: 2.1687494646464667, Val Loss: 1.869799207291513. Train Acc.: 0.34942287513116477, Val Acc.: 0.37358490566037733\n",
      "The model weight is saved based on val_acc: 0.3736\n",
      "The model weight is saved based on val_loss: 1.8698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2722cd496ea247b9910fe52c83405672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976a8ef33bb24d11a54b5fb9b56eed1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/150 - Train loss: 2.107718061925482, Val Loss: 1.8639586543137172. Train Acc.: 0.36621196222455404, Val Acc.: 0.3849056603773585\n",
      "The model weight is saved based on val_acc: 0.3849\n",
      "The model weight is saved based on val_loss: 1.8640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6442705d3f43d8a07a25ad8403fb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880f7f2ab3f64612b3cae455302edf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150 - Train loss: 2.059309301946746, Val Loss: 1.831354436334574. Train Acc.: 0.37985309548793283, Val Acc.: 0.4\n",
      "The model weight is saved based on val_acc: 0.4000\n",
      "The model weight is saved based on val_loss: 1.8314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af85b245d15245dca36237b07a1d2be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2294ebbe20984b1a908639c7fd7c3347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/150 - Train loss: 2.010642581571437, Val Loss: 1.7183841583863744. Train Acc.: 0.38903462749213014, Val Acc.: 0.39622641509433965\n",
      "The model weight is saved based on val_loss: 1.7184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849b986a89dd4bc1a41aceccc779be8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a70a686804a9ba5de157d378b754b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/150 - Train loss: 1.9593258444434822, Val Loss: 1.7156534968682056. Train Acc.: 0.40752885624344176, Val Acc.: 0.41509433962264153\n",
      "The model weight is saved based on val_acc: 0.4151\n",
      "The model weight is saved based on val_loss: 1.7157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22c768d48e498db8a69c821825c4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9379db4fce46b6a8199d45f1d1e0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150 - Train loss: 1.9083947620011577, Val Loss: 1.6898194061135345. Train Acc.: 0.41408709338929695, Val Acc.: 0.4188679245283019\n",
      "The model weight is saved based on val_acc: 0.4189\n",
      "The model weight is saved based on val_loss: 1.6898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3714ee187464dada29c729bfd17d8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa6c86e5244e20a42202b7458194d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/150 - Train loss: 1.8791028967934416, Val Loss: 1.7081445630991234. Train Acc.: 0.4226128016789087, Val Acc.: 0.4226415094339623\n",
      "The model weight is saved based on val_acc: 0.4226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7d6a899c9c4c11a6c47847abe7467a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d18515e3ecf403aaed179569f55e815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150 - Train loss: 1.8200002754847875, Val Loss: 1.6730261361823893. Train Acc.: 0.4420251836306401, Val Acc.: 0.4830188679245283\n",
      "The model weight is saved based on val_acc: 0.4830\n",
      "The model weight is saved based on val_loss: 1.6730\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcb8d9bf74143b88c78b0bec357a834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ff38675aeb4d298bb657c6ee34bc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/150 - Train loss: 1.787022522839269, Val Loss: 1.6448621286536163. Train Acc.: 0.4530430220356768, Val Acc.: 0.45660377358490567\n",
      "The model weight is saved based on val_loss: 1.6449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23c901a90e64de7a883347d10b37060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddc3724829948c9b2c9442027a4e9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150 - Train loss: 1.7497663985769747, Val Loss: 1.6406634182300208. Train Acc.: 0.4695697796432319, Val Acc.: 0.46037735849056605\n",
      "The model weight is saved based on val_loss: 1.6407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a892c60ed1e64e62be6f968cca88eeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 150\n",
    "train_losses, val_losses = [], []\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #prediction\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0) # loss.item() gives the average loss per image in the current batch\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    #validation phase\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss}, Val Loss: {val_loss}. Train Acc.: {train_acc}, Val Acc.: {val_acc}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), './weights/best_val_acc.pth')\n",
    "        print(f\"The model weight is saved based on val_acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/best_val_loss.pth')\n",
    "        print(f\"The model weight is saved based on val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe5ce-366b-48fa-a7cb-df1569da860e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (learnVenv)",
   "language": "python",
   "name": "learnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
