{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7d772-e6e4-463c-b09c-5b82071a988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as pd\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3c066-c41c-4b22-811b-0a3c34dc7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDataset(Dataset):\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.data = ImageFolder(dir_path, transform = transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832c029-b060-4cd2-ad65-31a969700476",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432bc16-9505-4d23-95b9-7c1c9bbcc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/../train'\n",
    "val_path = '/../valid/'\n",
    "\n",
    "train_data = CardDataset(train_path, transform = transform)\n",
    "val_data = CardDataset(val_path, transform = transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 32, num_workers=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size = 32, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531439b-0893-46ec-aced-5a53fe9649df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584185f-c795-4271-9c23-9118b4f527d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
    "                                               num_heads=n_heads,\n",
    "                                               dropout=dropout)\n",
    "        self.q = torch.nn.Linear(dim, dim)\n",
    "        self.k = torch.nn.Linear(dim, dim)\n",
    "        self.v = torch.nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(x, x, x)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03198c8b-0cd7-47a6-9810-b37acb7ba0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b35c06-2808-489d-87d5-aed83b4b4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eefd44-0f83-47b0-bdc3-cce69d18a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b33bf-bd9c-48ed-b292-3cf8274aa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit: https://www.youtube.com/watch?v=j3VNqtJUoz0&list=PLcpLsgRAryqx-dwIuJ9tT6BxJu8__LUUW&index=3\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, ch=3, img_size=224, patch_size=4, emb_dim=32,\n",
    "                n_layers=6, out_dim=53, dropout=0.1, heads=2): #out_dim=53, matching the number of classes\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
    "                                              patch_size=patch_size,\n",
    "                                              emb_size=emb_dim)\n",
    "        # Learnable params\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token to inputs\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "\n",
    "        # Output based on classification token\n",
    "        return self.head(x[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef38c5-8c93-4d6a-9baa-3ee94eb8400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4700782-5083-4393-9682-a12e04c86e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683322c-6424-462f-8ca6-98634f822187",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "train_losses, val_losses = [], []\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #prediction\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0) # loss.item() gives the average loss per image in the current batch\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    #validation phase\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss}, Val Loss: {val_loss}. Train Acc.: {train_acc}, Val Acc.: {val_acc}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), './weights/best_val_acc.pth')\n",
    "        print(f\"The model weight is saved based on val_acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/best_val_loss.pth')\n",
    "        print(f\"The model weight is saved based on val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe5ce-366b-48fa-a7cb-df1569da860e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (learnVenv)",
   "language": "python",
   "name": "learnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
